bias: true
block_size: 2048
gelu_approximate: tanh
head_size: 80
hf_config:
  name: phi-2
  org: microsoft
intermediate_size: 10240
lm_head_bias: true
mlp_class_name: GptNeoxMLP
n_embd: 2560
n_expert: 0
n_expert_per_token: 0
n_head: 32
n_layer: 32
n_query_groups: 32
name: phi-2
norm_class_name: LayerNorm
norm_eps: 1.0e-05
padded_vocab_size: 51200
padding_multiple: 512
parallel_residual: true
rope_base: 10000
rope_condense_ratio: 1
rotary_percentage: 0.4
scale_embeddings: false
shared_attention_norm: true
vocab_size: 50257
embed_choices:  [320,640,1280,2560]
head_choices: [8, 16, 32]
layer_choices: [30,31,32]
mlp_ratio_choices: [2, 3, 4]
bias_choices: [True, False]
_mlp_class: "GptNeoxMLP"
_norm_class: "LayerNorm"